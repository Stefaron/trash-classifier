# -*- coding: utf-8 -*-
"""trashClassification_Adamata.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19MwLEApWUI_LqZuZMrDqsXyM-6SHXdbW

##Install libraries

The first step in this project involved installing all necessary libraries. Key dependencies include wandb for experiment tracking, tensorflow for model development, and torchvision for data preprocessing. These libraries were chosen for their robust functionality and seamless integration with popular machine learning ecosystems. Leveraging these tools accelerated development and evaluation processes. However, dependency management relies on an updated environment and stable internet connectivity to avoid compatibility issues.
"""

# Install dependencies (if needed)
!pip install wandb datasets requests transformers torchvision huggingface_hub

import requests
import zipfile
import io
import wandb
import os
import numpy as np
import seaborn as sns
from PIL import Image
from matplotlib.colors import LinearSegmentedColormap
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.callbacks import ModelCheckpoint
from keras.layers import Input, Activation, Add, Dense, Conv2D, GlobalAveragePooling2D, MaxPooling2D
from keras.layers import BatchNormalization, Dropout
from tensorflow.keras.applications import ResNet50
from tensorflow.keras.applications.resnet50 import preprocess_input
from tensorflow.keras.optimizers import Adam
from keras.callbacks import ReduceLROnPlateau, EarlyStopping
from keras.models import Model
from keras.utils import plot_model
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.utils.class_weight import compute_class_weight

"""##Iniciate wandb"""

wandb.login()
wandb.init(project="trash-classification")

"""##Iniciate Dataset

The dataset used in this project was sourced from TrashNet, hosted on Hugging Face. This dataset contains images categorized by waste types. After downloading, a detailed analysis was conducted to ensure that all images were consistent in dimensions and ready for preprocessing. This initial step is crucial to ensure that the data pipeline runs smoothly, and it helps identify potential issues, such as non-standard image sizes, which could hinder model training.
"""

# URL file ZIP
url = "https://huggingface.co/datasets/garythung/trashnet/resolve/main/dataset-resized.zip"

# Mendownload file ZIP dari URL
response = requests.get(url)

# Mengekstrak file ZIP
with zipfile.ZipFile(io.BytesIO(response.content)) as zip_ref:
    zip_ref.extractall("/content")

"""##Analyze dataset

To verify dataset quality, sample images from each category were visualized. This step provided insights into the visual variations across different classes and helped identify potential issues like mislabeled or noisy images. Understanding these characteristics guided the design of relevant data augmentation strategies to enhance model performance. While effective, this step does not automatically detect outliers or imbalanced distributions, necessitating further in-depth analysis.
"""

data_dir = '/content/dataset-resized'

garbage_types = os.listdir(data_dir)

all_dimension_set = set()

for garbage_type in garbage_types:
  folder_path = os.path.join(data_dir, garbage_type)

  if os.path.isdir(folder_path):
    image_files = [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f)) and f.lower().endswith(('.png', '.jpg', '.jpeg'))]
    num_images = len(image_files)
    print(f"Folder: {garbage_type}, Number of Images: {num_images}")

    for image_file in image_files:
      image_path = os.path.join(folder_path, image_file)
      with Image.open(image_path) as img:
        width, height = img.size
        channels = len(img.getbands())
        dimension = (width, height, channels)
        all_dimension_set.add(dimension)

if len(all_dimension_set) == 1:
  width, height, channels = all_dimension_set.pop()
  print(f"\nAll images have the same dimensions: {width} x {height} and {channels} color channels.")
else:
  print("\nImages have different dimensions or color channels.")

for garbage_type in garbage_types:
    folder_path = os.path.join(data_dir, garbage_type)

    # Verify that the current item is a directory
    if os.path.isdir(folder_path):
        image_files = [f for f in os.listdir(folder_path) if f.endswith(('jpg', 'jpeg'))]

        # Select the first 10 images
        image_files = image_files[:7]

        # Set up subplots
        fig, axs = plt.subplots(1, 7, figsize=(15, 2))

        for i, image_file in enumerate(image_files):
            image_path = os.path.join(folder_path, image_file)
            with Image.open(image_path) as img:
                axs[i].imshow(img)
                axs[i].axis('off')

        plt.tight_layout()
        fig.suptitle(garbage_type, fontsize=20, y=1.03)
        plt.show()

"""##Preprocessing data

The dataset was split into training and validation sets using stratified sampling to preserve class proportions. Data augmentation techniques, such as rotation, zooming, and flipping, were applied to improve the model’s generalization capabilities when faced with real-world variations. Additionally, preprocessing steps like normalization were implemented to expedite model convergence. While augmentation strengthens model robustness, improper parameter settings can introduce excessive noise, potentially degrading performance.
"""

data = []

# Loop through each garbage type and collect its images' file paths
for garbage_type in garbage_types:
    # Check if it's a directory before processing
    garbage_type_path = os.path.join(data_dir, garbage_type)
    if os.path.isdir(garbage_type_path):
        for file in os.listdir(garbage_type_path):
            # Append the image file path and its trash type (as a label) to the data list
            data.append((os.path.join(garbage_type_path, file), garbage_type))


# Convert the collected data into a DataFrame
df = pd.DataFrame(data, columns=['filepath', 'label'])

# Display the first few entries of the DataFrame
df.head()

# Split with stratification
train_df, val_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['label'])

# Print the number of images in each set
print(f"Number of images in the training set: {len(train_df)}")
print(f"Number of images in the validation set: {len(val_df)}")

# 1. Class distribution in the entire dataset
overall_distribution = df['label'].value_counts(normalize=True) * 100

# 2. Class distribution in the training set
train_distribution = train_df['label'].value_counts(normalize=True) * 100

# 3. Class distribution in the validation set
val_distribution = val_df['label'].value_counts(normalize=True) * 100

print("Class distribution in the entire dataset:\n")
print(overall_distribution.round(2))
print('-'*40)

print("\nClass distribution in the training set:\n")
print(train_distribution.round(2))
print('-'*40)

print("\nClass distribution in the validation set:\n")
print(val_distribution.round(2))

# Slight Augmentation settings for training
train_datagen = ImageDataGenerator(
    rotation_range=60,                  # Randomly rotate the images by up to 60 degrees
    width_shift_range=0.15,             # Randomly shift images horizontally by up to 15% of the width
    height_shift_range=0.15,            # Randomly shift images vertically by up to 15% of the height
    zoom_range=0.20,                    # Randomly zoom in or out by up to 20%
    horizontal_flip=True,               # Randomly flip images horizontally
    vertical_flip=True,                 # Randomly flip images vertically
    shear_range=0.05,                   # Apply slight shear transformations
    brightness_range=[0.9, 1.1],        # Vary brightness between 90% to 110% of original
    channel_shift_range=10,             # Randomly shift channels (can change colors of images slightly but less aggressively)
    fill_mode='nearest',                 # Fill in missing pixels using the nearest filled value
    preprocessing_function=preprocess_input  # Add this line
)

# For the validation set, you might not have augmentation:
val_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)  # Add this line

# Using flow_from_dataframe to generate batches
train_generator = train_datagen.flow_from_dataframe(
    dataframe=train_df,                  # DataFrame containing training data
    x_col="filepath",                    # Column with paths to image files
    y_col="label",                       # Column with image labels
    target_size=(384, 384),              # Resize all images to size of 384x384
    batch_size=32,                       # Number of images per batch
    class_mode='categorical',            # One-hot encode labels
    seed=42,                             # Seed for random number generator to ensure reproducibility
    shuffle=False                        # Data is not shuffled; order retained from DataFrame
)


# Generate validation batches from the validation dataframe
val_generator = val_datagen.flow_from_dataframe(
    dataframe=val_df,                    # DataFrame containing validation data
    x_col="filepath",                    # Column with paths to image files
    y_col="label",                       # Column with image labels
    target_size=(384, 384),              # Resize all images to size of 384x384
    batch_size=32,                       # Number of images per batch
    class_mode='categorical',            # One-hot encode labels
    seed=42,                             # Seed for random number generator to ensure reproducibility
    shuffle=False                        # Data is not shuffled; order retained from DataFrame
)

print(f"Number of batches in train_generator: {len(train_generator)}")
print(f"Number of batches in val_generator: {len(val_generator)}")

"""##Dealing with imbalanced datasets

Class distribution analysis revealed significant imbalance, with certain classes having fewer samples. To address this, class weights were calculated and applied during training to ensure that minority classes were equally prioritized. This approach is effective for mitigating bias in model predictions, but its success depends on having a sufficient number of samples in minority classes to provide meaningful learning signals.
"""

class_labels = train_df['label'].unique()
class_labels

train_generator.class_indices

weights = compute_class_weight(class_weight='balanced', classes=class_labels, y=train_df['label'])
weights

class_weights = dict(zip(train_generator.class_indices.values(), weights))
class_weights

"""##Build model with Transfer Learning

The chosen architecture leveraged ResNet50, a pretrained model trained on the ImageNet dataset. Early layers of the model were frozen to retain pre-learned features, while the final layers were fine-tuned to classify waste categories. Transfer learning was chosen for its ability to accelerate training and achieve high accuracy with limited data. However, freezing too many layers could limit the model’s adaptability to the new domain, which necessitates careful tuning of trainable layers.
"""

# Load the ResNet50 model with weights pre-trained on ImageNet
base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(384, 384, 3))

base_model.summary()

len(base_model.layers)

for i, layer in enumerate(base_model.layers):
    if 140 <= i <= 150:
        print(i, layer.name)

for layer in base_model.layers[:143]: # include the layer 142
    layer.trainable = False

# Create the new model
x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dropout(0.5)(x)
x = Dense(6, activation='softmax')(x)

model = Model(inputs=base_model.input, outputs=x)

model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])

model.summary()

"""##Define Callback and Training Model

The model was trained using the Adam optimizer, along with callbacks such as early stopping, learning rate reduction, and checkpointing the best-performing model based on validation loss. These callbacks prevent overfitting, ensure efficient resource usage, and preserve the most optimal version of the model. However, there is a risk that early stopping might terminate training prematurely before the model fully converges.
"""

# Define the callbacks
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=0.00001)
early_stopping = EarlyStopping(monitor='val_loss', mode='min', patience=8, restore_best_weights=True, verbose=1)
model_checkpoint = ModelCheckpoint(
    filepath="best_model.keras",  # Nama file untuk menyimpan model terbaik
    monitor="val_loss",       # Metric yang dipantau (val_loss dalam kasus ini)
    mode="min",               # 'min' karena kita ingin val_loss serendah mungkin
    save_best_only=True,      # Hanya menyimpan model terbaik
    verbose=1                 # Menampilkan log ketika model disimpan
)

# Callback list
callbacks = [reduce_lr, early_stopping, model_checkpoint]

# Total number of epochs
num_epochs = 50

# Train the model
history = model.fit(train_generator,
                    epochs=num_epochs,
                    validation_data=val_generator,
                    class_weight=class_weights,
                    callbacks=callbacks)

"""##Model Evaluation

Model performance was assessed using metrics such as accuracy and loss curves, along with a classification report and confusion matrix. This evaluation provided a detailed understanding of how well the model handled each waste category and identified any bias toward specific classes. Visualizations like confusion matrices highlighted areas for improvement. Interpreting these metrics requires expertise to fully assess the model’s strengths and areas for refinement.
"""

def plot_learning_curves(history, start_epoch=5):
    """
    Plot training and validation loss and accuracy curves.

    Parameters:
    - history: Training history (output from the model's fit method).
    - start_epoch: Epoch from which to start plotting. Default is 5 (i.e., plot from epoch 6 onwards).
    """

    # Get the minimum length among all metric lists
    min_len = min(len(history.history[key]) for key in history.history)

    # Truncate all metric lists to the minimum length
    truncated_history = {key: history.history[key][:min_len] for key in history.history}

    # Convert the truncated history to a pandas DataFrame
    df = pd.DataFrame(truncated_history)

    # Plot the curves from the specified epoch onwards
    df = df.iloc[start_epoch-1:]

    # Set the style of seaborn for better visualization
    sns.set(rc={'axes.facecolor': '#f0f0fc'}, style='darkgrid')

    # Plotting the learning curves
    plt.figure(figsize=(15,6))

    # Plotting the training and validation loss
    plt.subplot(1, 2, 1)
    sns.lineplot(x=df.index, y=df['loss'], color='royalblue', label='Train Loss')
    sns.lineplot(x=df.index, y=df['val_loss'], color='orangered', linestyle='--', label='Validation Loss')
    plt.title('Loss Evolution')

    # Plotting the training and validation accuracy
    plt.subplot(1, 2, 2)
    sns.lineplot(x=df.index, y=df['accuracy'], color='royalblue', label='Train Accuracy')
    sns.lineplot(x=df.index, y=df['val_accuracy'], color='orangered', linestyle='--', label='Validation Accuracy')
    plt.title('Accuracy Evolution')

    plt.show()

def evaluate_model_performance(model, val_generator, class_labels):
    """
    Evaluate the model's performance on the validation set and print the classification report.

    Parameters:
    - model: The trained model.
    - val_generator: Validation data generator.
    - class_labels: List of class names.

    Returns:
    - report: Classification report as a string.
    """

    # Getting all the true labels for the validation set
    true_labels = val_generator.classes

    # Get the class labels (names) from the generator
    class_labels = list(val_generator.class_indices.keys())

    # To get the predicted labels, we predict using the model
    predictions = model.predict(val_generator, steps=len(val_generator))

    # Take the argmax to get the predicted class indices.
    predicted_labels = np.argmax(predictions, axis=1)

    # Extracting true labels from the validation generator
    true_labels = val_generator.classes

    # Classification report
    report = classification_report(true_labels, predicted_labels, target_names=class_labels)
    print(report)
    print('\n')

    # Define a custom colormap
    colors = ["white", "royalblue"]
    cmap_cm = LinearSegmentedColormap.from_list("cmap_cm", colors)

    # Confusion Matrix
    cm = confusion_matrix(true_labels, predicted_labels)

    # Plotting confusion matrix using seaborn
    plt.figure(figsize=(8,6))
    sns.heatmap(cm, annot=True, cmap=cmap_cm, fmt='d', xticklabels=class_labels, yticklabels=class_labels)
    plt.xlabel('Predicted Labels')
    plt.ylabel('True Labels')
    plt.title('Confusion Matrix')
    plt.show()

plot_learning_curves(history, start_epoch=1)

evaluate_model_performance(model, val_generator, class_labels)